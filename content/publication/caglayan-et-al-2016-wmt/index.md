---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Does Multimodality Help Human and Machine for Translation and Image Captioning?
subtitle: ''
summary: ''
authors:
- Ozan Caglayan
- Walid Aransa
- Yaxing Wang
- Marc Masana
- Mercedes Garcı́a-Mart\ńez
- Fethi Bougares
- Loı̈c Barrault
- Joost van de Weijer
tags: ['Multimodal MT', 'WMT']
categories: []
date: '2016-08-01'
lastmod: 2021-02-06T00:00:32Z
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-02-06T00:00:32.384909Z'
publication_types:
- '1'
abstract: This paper presents the systems developed by LIUM and CVC for the WMT16
  Multimodal Machine Translation challenge. We explored various comparative methods,
  namely phrase-based systems and attentional recurrent neural networks models trained
  using monomodal or multimodal data. We also performed a human evaluation in order
  to estimate the usefulness of multimodal data for human machine translation and
  image description generation. Our systems obtained the best results for both tasks
  according to the automatic evaluation metrics BLEU and METEOR.
publication: '*Proceedings of the First Conference on Machine Translation*'
url_pdf: https://arxiv.org/pdf/1605.09186
---
