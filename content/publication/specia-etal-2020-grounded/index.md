---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Grounded Sequence to Sequence Transduction
subtitle: ''
summary: ''
authors:
- L. Specia
- L. Barrault
- O. Caglayan
- A. Duarte
- D. Elliott
- S. Gella
- N. Holzenberger
- C. Lala
- S. J. Lee
- J. Libovicky
- P. Madhyastha
- F. Metze
- K. Mulligan
- A. Ostapenko
- S. Palaskar
- R. Sanabria
- J. Wang
- R. Arora
tags: []
categories: []
date: '2020-03-01'
lastmod: 2022-04-28T21:07:55Z
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-04-28T21:07:53.483688Z'
publication_types:
- '2'
abstract: Speech recognition and machine translation have made major progress over
  the past decades, providing practical systems to map one language sequence to another.
  Although multiple modalities such as sound and video are becoming increasingly available,
  the state-of-the-art systems are inherently unimodal, in the sense that they take
  a single modality - either speech or text - as input. Evidence from human learning
  suggests that additional modalities can provide disambiguating signals crucial for
  many language tasks. In this article, we describe the How2 dataset , a large, open-domain
  collection of videos with transcriptions and their translations. We then show how
  this single dataset can be used to develop systems for a variety of language tasks
  and present a number of models meant as starting points. Across tasks, we find that
  building multimodal architectures that perform better than their unimodal counterpart
  remains a challenge. This leaves plenty of room for the exploration of more advanced
  solutions that fully exploit the multimodal nature of the How2 dataset , and the
  general direction of multimodal learning with other datasets as well.
publication: '*IEEE Journal of Selected Topics in Signal Processing*'
url_pdf: http://ufal.mff.cuni.cz/biblio/attachments/2020-libovicky-p4396153103180475356.pdf
doi: 10.1109/JSTSP.2020.2998415
---
