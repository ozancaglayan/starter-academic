---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Cross-lingual Visual Pre-training for Multimodal Machine Translation
subtitle: ''
summary: ''
authors:
- Ozan Caglayan
- Menekse Kuyu
- Mustafa Sercan Amac
- Pranava Madhyastha
- Erkut Erdem
- Aykut Erdem
- Lucia Specia
tags: ['Multimodal MT', 'Pre-training']
date: '2021-04-01'
lastmod: 2021-04-19T11:28:56Z
featured: true
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-04-19T11:28:56.084143Z'
publication_types:
- '1'
abstract: Pre-trained language models have been shown to improve performance in many
  natural language tasks substantially. Although the early focus of such models was
  single language pre-training, recent advances have resulted in cross-lingual and
  visual pre-training methods. In this paper, we combine these two approaches to learn
  visually-grounded cross-lingual representations. Specifically, we extend the translation
  language modelling (Lample and Conneau, 2019) with masked region classification
  and perform pre-training with three-way parallel vision & language corpora. We show
  that when fine-tuned for multimodal machine translation, these models obtain state-of-the-art
  performance. We also provide qualitative insights into the usefulness of the learned
  grounded representations.
publication: '*Proceedings of the 16th Conference of the European Chapter of the Association
  for Computational Linguistics: Main Volume*'
url_pdf: https://www.aclweb.org/anthology/2021.eacl-main.112.pdf
url_code: https://github.com/ImperialNLP/VTLM
---
