---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'How2: A Large-scale Dataset For Multimodal Language Understanding'
subtitle: ''
summary: ''
authors:
- Ramon Sanabria
- Ozan Caglayan
- Shruti Palaskar
- Desmond Elliott
- LÃ¶ic Barrault
- Lucia Specia
- Florian Metze
tags: []
categories: []
date: '2018-12-01'
lastmod: 2021-02-05T23:49:01Z
featured: true
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-02-05T23:49:00.880361Z'
publication_types:
- '1'
abstract: In this paper, we introduce How2, a multimodal collection of instructional
  videos with English subtitles and crowdsourced Portuguese translations. We also
  present integrated sequence-to-sequence baselines for machine translation, automatic
  speech recognition, spoken language translation, and multimodal summarization. By
  making available data and code for several multimodal natural language tasks, we
  hope to stimulate more research on these and similar challenges, to obtain a deeper
  understanding of multimodality in language processing.
publication: '*Proceedings of the Workshop on Visually Grounded Interaction and Language
  (NeurIPS 2018)*'
url_pdf: http://arxiv.org/abs/1811.00347
---
