---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation
subtitle: ''
summary: ''
authors:
- Julia Ive
- Andy Mingren Li
- Yishu Miao
- Ozan Caglayan
- Pranava Madhyastha
- Lucia Specia
tags: ['Multimodal MT', 'Simultaneous MT', 'Reinforcement Learning']
categories: []
date: '2021-04-01'
lastmod: 2021-04-19T11:28:56Z
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-04-19T11:28:56.213517Z'
publication_types:
- '1'
abstract: 'This paper addresses the problem of simultaneous machine translation (SiMT)
  by exploring two main concepts: (a) adaptive policies to learn a good trade-off
  between high translation quality and low latency; and (b) visual information to
  support this process by providing additional (visual) contextual information which
  may be available before the textual input is produced. For that, we propose a multimodal
  approach to simultaneous machine translation using reinforcement learning, with
  strategies to integrate visual and textual information in both the agent and the
  environment. We provide an exploration on how different types of visual information
  and integration strategies affect the quality and latency of simultaneous translation
  models, and demonstrate that visual cues lead to higher quality while keeping the
  latency low.'
publication: '*Proceedings of the 16th Conference of the European Chapter of the Association
  for Computational Linguistics: Main Volume*'
url_pdf: https://www.aclweb.org/anthology/2021.eacl-main.281.pdf
url_code: https://github.com/ImperialNLP/sim-mt
---
