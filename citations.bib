%@inproceedings{ccauglayan2012p300,
  %title={P300 based auditory visual brain computer interface},
  %author={{\c{C}}a{\u{g}}layan, Ozan and Arslan, Reis Burak},
  %booktitle={Signal Processing and Communications Applications Conference (SIU), 2012 20th},
  %pages={1--4},
  %year={2012},
  %organization={IEEE}
%}

%@inproceedings{saylam2014analysis,
  %title={Analysis of visual stimulus parameters for SSVEP based brain computer interfaces},
  %author={Saylam, Berrenur and Caglayan, Ozan and Arslan, Reis Burak},
  %booktitle={Biomedical Engineering Meeting (BIYOMUT), 2014 18th National},
  %pages={1--4},
  %year={2014},
  %organization={IEEE}
%}

%@inproceedings{caglayan2013robotic,
  %title={ROBOTIC ARM CONTROL WITH BRAIN COMPUTER INTERFACE USING P300 AND SSVEP},
  %author={Caglayan, Ozan and Arslan, Reis Burak},
  %booktitle={IASTED International Conference Biomedical Engineering (BioMed 2013)},
  %pages={141--144},
  %year={2013},
  %organization={ACTA Press}
%}

@inproceedings{caglayan2013humanoid,
  title={Humanoid Robot Control with {SSVEP} on Embedded System},
  author={Caglayan, Ozan and Arslan, Reis Burak},
  booktitle={BCI Meeting 2013},
  address={California, USA},
  month={June},
  abstract={In this study we implemented a low-cost, single EEG channel brain computer interface (BCI) running on an embedded computer. The BCI uses steady-state visual evoked potentials (SSVEP) to control the motion of the Kondo KHR-3HV humanoid robot. The subject attends to one of the flickering LEDs attached to the arms of the robot in order to move an arm. SSVEPs are identified by a simple spectrum analysis of the EEG signal.},
  url={http://dx.doi.org/10.3217/978-3-85125-260-6-129},
  year={2013}
}

@InProceedings{caglayan-EtAl:2016:WMT,
  author    = {Caglayan, Ozan  and  Aransa, Walid  and  Wang, Yaxing  and  Masana, Marc  and  Garc{\'\i}a-Mart{\'\i}nez, Mercedes  and  Bougares, Fethi  and  Barrault, Lo{\"\i}c  and  van de Weijer, Joost},
  title     = {Does Multimodality Help Human and Machine for Translation and Image Captioning?},
  booktitle = {Proceedings of the First Conference on Machine Translation},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {627--633},
  abstract  = {This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.},
  url       = {https://arxiv.org/pdf/1605.09186}
}

@article{caglayan2016multimodal,
  title={Multimodal Attention for Neural Machine Translation},
  author={Caglayan, Ozan and Barrault, Lo{\"\i}c and Bougares, Fethi},
  journal={arXiv preprint arXiv:1609.03976},
  url={https://arxiv.org/pdf/1609.03976},
  abstract={The attention mechanism is an important part of the neural machine translation (NMT) where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30k multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline.},
  month={September},
  year={2016}
}

@article{caglayan2017nmtpy,
  title={{NMTPY}: A Flexible Toolkit for Advanced Neural Machine Translation Systems},
  author={Caglayan, Ozan and Garc{\'\i}a-Mart{\'\i}nez, Mercedes and Bardet, Adrien and Aransa, Walid and Bougares, Fethi and Barrault, Lo{\"\i}c},
  journal={The Prague Bulletin of Mathematical Linguistics},
  volume={109},
  number={1},
  pages={15--28},
  year={2017},
  month={September},
  url={https://ufal.mff.cuni.cz/pbml/109/art-caglayan-et-al.pdf},
  abstract={In this paper, we present nmtpy, a flexible Python toolkit based on Theano for training Neural Machine Translation and other neural sequence-to-sequence architectures. nmtpy decouples the specification of a network from the training and inference utilities to simplify the addition of a new architecture and reduce the amount of boilerplate code to be written. nmtpy has been used for LIUMâ€™s top-ranked submissions to WMT Multimodal Machine Translation and News Translation tasks in 2016 and 2017.},
  publisher={De Gruyter Open}
}

@InProceedings{caglayan-EtAl:2017:WMT,
  author    = {Caglayan, Ozan  and  Aransa, Walid  and  Bardet, Adrien  and  Garc{\'\i}a-Mart{\'\i}nez, Mercedes  and  Bougares, Fethi  and  Barrault, Lo{\"\i}c  and  Masana, Marc  and  Herranz, Luis  and  van de Weijer, Joost},
  title     = {LIUM-CVC Submissions for WMT17 Multimodal Translation Task},
  booktitle = {Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {432--439},
  abstract  = {This paper describes the monomodal and multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT17 Shared Task on Multimodal Translation. We mainly explored two multimodal architectures where either global visual features or convolutional feature maps are integrated in order to benefit from visual context. Our final systems ranked first for both En-De and En-Fr language pairs according to the automatic evaluation metrics METEOR and BLEU.},
  url       = {https://arxiv.org/pdf/1707.04481}
}

@InProceedings{garciamartinez-EtAl:2017:WMT,
  author    = {Garc{\'\i}a-Mart{\'\i}nez, Mercedes  and  Caglayan, Ozan  and  Aransa, Walid  and  Bardet, Adrien  and  Bougares, Fethi  and  Barrault, Lo{\"\i}c},
  title     = {LIUM Machine Translation Systems for WMT17 News Translation Task},
  booktitle = {Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {288--295},
  abstract  = {This paper describes LIUM submissions to WMT17 News Translation Task for English-German, English-Turkish, English-Czech and English-Latvian language pairs. We train BPE-based attentive Neural Machine Translation systems with and without factored outputs using the open source nmtpy framework. Competitive scores were obtained by ensembling various systems and exploiting the availability of target monolingual corpora for back-translation. The impact of back-translation quantity and quality is also analyzed for English-Turkish where our post-deadline submission surpassed the best entry by +1.6 BLEU.},
  url       = {https://arxiv.org/pdf/1707.04499}
}

@article{rougier2017sustainable,
  title={Sustainable Computational Science: The {ReScience} Initiative},
  author={Rougier, Nicolas P and Hinsen, Konrad and Alexandre, Fr{\'e}d{\'e}ric and Arildsen, Thomas and Barba, Lorena A and Benureau, Fabien CY and Brown, C Titus and De Buyl, Pierre and Caglayan, Ozan and Davison, Andrew P and others},
  journal={PeerJ Computer Science},
  abstract={Computer science offers a large set of tools for prototyping, writing, running, testing, validating, sharing and reproducing results; however, computational science lags behind. In the best case, authors may provide their source code as a compressed archive and they may feel confident their research is reproducible. But this is not exactly true. James Buckheit and David Donoho proposed more than two decades ago that an article about computational results is advertising, not scholarship. The actual scholarship is the full software environment, code, and data that produced the result. This implies new workflows, in particular in peer-reviews. Existing journals have been slow to adapt: source codes are rarely requested and are hardly ever actually executed to check that they produce the results advertised in the article. ReScience is a peer-reviewed journal that targets computational research and encourages the explicit replication of already published research, promoting new and open-source implementations in order to ensure that the original research can be replicated from its description. To achieve this goal, the whole publishing chain is radically different from other traditional scientific journals. ReScience resides on GitHub where each new implementation of a computational study is made available together with comments, explanations, and software tests.},
  volume={3},
  pages={e142},
  year={2017},
  month={December},
  url={https://doi.org/10.7717/peerj-cs.142},
  publisher={PeerJ Inc.}
}

@InProceedings{caglayan-EtAl:2018:WMT,
  author    = {Caglayan, Ozan  and  Bardet, Adrien  and  Bougares, Fethi  and  Barrault, Lo{\"\i}c and  Wang, Kai  and  Masana, Marc  and  Herranz, Luis  and  van de Weijer, Joost},
  title     = {LIUM-CVC Submissions for WMT18 Multimodal Translation Task},
  booktitle = {Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers},
  month     = {October},
  year      = {2018},
  address   = {Belgium, Brussels},
  publisher = {Association for Computational Linguistics},
  pages     = {603--608},
  abstract  = {This paper describes the multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT18 Shared Task on Multimodal Translation. This year we propose several modifications to our previous multimodal attention architecture in order to better integrate convolutional features and refine them using encoder-side information. Our final constrained submissions ranked first for English-French and second for English-German language pairs among the constrained submissions according to the automatic evaluation metric METEOR.},
  url       = {https://arxiv.org/pdf/1809.00151}
}

@inproceedings{sanabria-etal-2018-how2,
  title = {{How2:} A Large-scale Dataset For Multimodal Language Understanding},
  author = {Sanabria, Ramon and Caglayan, Ozan and Palaskar, Shruti and Elliott, Desmond and Barrault, Lo\"{i}c and Specia, Lucia and Metze, Florian},
  month = {December},
  booktitle={Proceedings of the Workshop on Visually Grounded Interaction and Language (NeurIPS 2018)},
  address={Montreal, Canada},
  year = {2018},
  abstract={In this paper, we introduce How2, a multimodal collection of instructional videos with English subtitles and crowdsourced Portuguese translations. We also present integrated sequence-to-sequence baselines for machine translation, automatic speech recognition, spoken language translation, and multimodal summarization. By making available data and code for several multimodal natural language tasks, we hope to stimulate more research on these and similar challenges, to obtain a deeper understanding of multimodality in language processing.},
  url = {http://arxiv.org/abs/1811.00347}
}

@INPROCEEDINGS{caglayan-etal-2019-multimodal,
  author={O. {Caglayan} and R. {Sanabria} and S. {Palaskar} and L. {Barraul} and F. {Metze}},
  booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title={Multimodal Grounding for Sequence-to-sequence Speech Recognition},
  year={2019},
  volume={},
  number={},
  pages={8648-8652},
  abstract={Humans are capable of processing speech by making use of multiple sensory modalities. For example, the environment where a conversation takes place generally provides semantic and/or acoustic context that helps us to resolve ambiguities or to recall named entities. Motivated by this, there have been many works studying the integration of visual information into the speech recognition pipeline. Specifically, in our previous work, we propose a multistep visual adaptive training approach which improves the accuracy of an audio-based Automatic Speech Recognition (ASR) system. This approach, however, is not end-to-end as it requires fine-tuning the whole model with an adaptation layer. In this paper, we propose novel end-to-end multimodal ASR systems and compare them to the adaptive approach by using a range of visual representations obtained from state-of-the-art convolutional neural networks. We show that adaptive training is effective for S2S models leading to an absolute improvement of 1.4% in word error rate. As for the end-to-end systems, although they perform better than baseline, the improvements are slightly less than adaptive training, 0.8 absolute WER reduction in single-best models. Using ensemble decoding, end-to-end models reach a WER of 15% which is the lowest score among all systems.},
  doi={10.1109/ICASSP.2019.8682750},
  ISSN={2379-190X},
  month={May},
  url={https://arxiv.org/pdf/1811.03865}
}

@inproceedings{caglayan-etal-2019-probing,
    title = "Probing the Need for Visual Context in Multimodal Machine Translation",
    author = {Caglayan, Ozan  and
      Madhyastha, Pranava  and
      Specia, Lucia  and
      Barrault, Lo{\"\i}c},
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1422.pdf",
    doi = "10.18653/v1/N19-1422",
    pages = "4159--4170",
    abstract = "Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.",
}

@inproceedings{wu-etal-2019-transformer,
	title = {Transformer-based Cascaded Multimodal Speech Translation},
	author = {Wu, Zixiu and Caglayan, Ozan and Ive, Julia and Wang, Josiah and Specia, Lucia},
	booktitle = {Proceedings of the 16th International Workshop on Spoken Language Translation ({IWSLT})},
	address = {Hong Kong},
	year = {2019},
	url = {https://arxiv.org/pdf/1910.13215},
	abstract = "This paper describes the cascaded multimodal speech translation systems developed by Imperial College London for the IWSLT 2019 evaluation campaign. The architecture consists of an automatic speech recognition (ASR) system followed by a Transformer-based multimodal machine translation (MMT) system. While the ASR component is identical across the experiments, the MMT model varies in terms of the way of integrating the visual context (simple conditioning vs. attention), the type of visual features exploited (pooled, convolutional, action categories) and the underlying architecture. For the latter, we explore both the canonical transformer and its deliberation version with additive and cascade variants which differ in how they integrate the textual attention. Upon conducting extensive experiments, we found that (i) the explored visual integration schemes often harm the translation performance for the transformer and additive deliberation, but considerably improve the cascade deliberation;(ii) the transformer and cascade deliberation integrate the visual modality better than the additive deliberation, as shown by the incongruence analysis."
}

@article{sulubacak-etal-2020-multimodal,
  title={Multimodal machine translation through visuals and speech},
  author={Sulubacak, Umut and Caglayan, Ozan and Gr{\"o}nroos, Stig-Arne and Rouhe, Aku and Elliott, Desmond and Specia, Lucia and Tiedemann, J{\"o}rg},
  journal={Machine Translation},
  volume={34},
  number={2},
  pages={97--147},
  year={2020},
  url={https://arxiv.org/pdf/1911.12798},
  abstract= "Multimodal machine translation involves drawing information from more than one modality, based on the assumption that the additional modalities will contain useful alternative views of the input data. The most prominent tasks in this area are spoken language translation, image-guided translation, and video-guided translation, which exploit audio and visual modalities, respectively. These tasks are distinguished from their monolingual counterparts of speech recognition, image captioning, and video captioning by the requirement of models to generate outputs in a different language. This survey reviews the major data resources for these tasks, the evaluation campaigns concentrated around them, the state of the art in end-to-end and pipeline approaches, and also the challenges in performance evaluation. The paper concludes with a discussion of directions for future research in these areas: the need for more expansive and challenging datasets, for targeted evaluations of model performance, and for multimodality in both the input and output space.",
  publisher={Springer}
}

@ARTICLE{specia-etal-2020-grounded,
  author={L. {Specia} and L. {Barrault} and O. {Caglayan} and A. {Duarte} and D. {Elliott} and S. {Gella} and N. {Holzenberger} and C. {Lala} and S. J. {Lee} and J. {Libovicky} and P. {Madhyastha} and F. {Metze} and K. {Mulligan} and A. {Ostapenko} and S. {Palaskar} and R. {Sanabria} and J. {Wang} and R. {Arora}},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  title={Grounded Sequence to Sequence Transduction},
  year={2020},
  volume={14},
  number={3},
  pages={577-591},
  abstract={Speech recognition and machine translation have made major progress over the past decades, providing practical systems to map one language sequence to another. Although multiple modalities such as sound and video are becoming increasingly available, the state-of-the-art systems are inherently unimodal, in the sense that they take a single modality - either speech or text - as input. Evidence from human learning suggests that additional modalities can provide disambiguating signals crucial for many language tasks. In this article, we describe the How2 dataset , a large, open-domain collection of videos with transcriptions and their translations. We then show how this single dataset can be used to develop systems for a variety of language tasks and present a number of models meant as starting points. Across tasks, we find that building multimodal architectures that perform better than their unimodal counterpart remains a challenge. This leaves plenty of room for the exploration of more advanced solutions that fully exploit the multimodal nature of the How2 dataset , and the general direction of multimodal learning with other datasets as well.},
  doi={10.1109/JSTSP.2020.2998415},
  ISSN={1941-0484},
  month={March},
  url={http://ufal.mff.cuni.cz/biblio/attachments/2020-libovicky-p4396153103180475356.pdf}

@inproceedings{caglayan-etal-2020-simultaneous,
    title = "Simultaneous Machine Translation with Visual Context",
    author = {Caglayan, Ozan  and
      Ive, Julia  and
      Haralampieva, Veneta  and
      Madhyastha, Pranava  and
      Barrault, Lo{\"\i}c  and
      Specia, Lucia},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.184.pdf",
    doi = "10.18653/v1/2020.emnlp-main.184",
    pages = "2350--2361",
    abstract = "Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French.",
}

@inproceedings{caglayan-etal-2020-curious,
    title = "Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale",
    author = "Caglayan, Ozan  and
      Madhyastha, Pranava  and
      Specia, Lucia",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.210.pdf",
    doi = "10.18653/v1/2020.coling-main.210",
    pages = "2322--2328",
    abstract = "Automatic evaluation of language generation systems is a well-studied problem in Natural Language Processing. While novel metrics are proposed every year, a few popular metrics remain as the de facto metrics to evaluate tasks such as image captioning and machine translation, despite their known limitations. This is partly due to ease of use, and partly because researchers expect to see them and know how to interpret them. In this paper, we urge the community for more careful consideration of how they automatically evaluate their models by demonstrating important failure cases on multiple datasets, language pairs and tasks. Our experiments show that metrics (i) usually prefer system outputs to human-authored texts, (ii) can be insensitive to correct translations of rare words, (iii) can yield surprisingly high scores when given a single sentence as system output for the entire test set.",
}

@inproceedings{caglayan-etal-2021-cross,
    title = "Cross-lingual Visual Pre-training for Multimodal Machine Translation",
    author = "Caglayan, Ozan  and
      Kuyu, Menekse  and
      Amac, Mustafa Sercan  and
      Madhyastha, Pranava  and
      Erdem, Erkut  and
      Erdem, Aykut  and
      Specia, Lucia",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.eacl-main.112",
    pages = "1317--1324",
    abstract = "Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded cross-lingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision {\&} language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain state-of-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.",
}

@inproceedings{ive-etal-2021-exploiting,
    title = "Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation",
    author = "Ive, Julia  and
      Li, Andy Mingren  and
      Miao, Yishu  and
      Caglayan, Ozan  and
      Madhyastha, Pranava  and
      Specia, Lucia",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.eacl-main.281",
    pages = "3222--3233",
    abstract = "This paper addresses the problem of simultaneous machine translation (SiMT) by exploring two main concepts: (a) adaptive policies to learn a good trade-off between high translation quality and low latency; and (b) visual information to support this process by providing additional (visual) contextual information which may be available before the textual input is produced. For that, we propose a multimodal approach to simultaneous machine translation using reinforcement learning, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low.",
}
